{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epsilon_greedy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaehanKim/reinforcement_learning_pytorch/blob/master/epsilon_greedy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjq659wfTKMQ",
        "colab_type": "text"
      },
      "source": [
        "# Epsilon Greedy Algorithm\n",
        "\n",
        "This notebook solves MAB(Multi-armed bandit) problem with epsilon greedy algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqkyVUd6ExME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def pull_bandit(action_tensor):\n",
        "    reward_mask = np.random.randn(action_tensor.size(0)) > bandit[action_tensor]\n",
        "    reward = torch.empty(action_tensor.size(0), dtype=torch.int32)\n",
        "    reward[reward_mask] = 1\n",
        "    reward[~reward_mask] = 0\n",
        "    return reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2a_kUcZFF_M",
        "colab_type": "code",
        "outputId": "d1cbd26b-9f2b-42b9-eaf4-d48eec624999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# config\n",
        "NUM_BANDIT = 8\n",
        "T = 10000\n",
        "LR = 1e-3\n",
        "eps = .1\n",
        "n_batch = 100\n",
        "LOG_INT = 1000\n",
        "\n",
        "# define bandit\n",
        "bandit = np.random.randn(NUM_BANDIT)\n",
        "\n",
        "# learnable parameter\n",
        "empherical_prob = torch.FloatTensor(NUM_BANDIT).uniform_(0,1)\n",
        "\n",
        "\n",
        "# sgd = torch.optim.SGD([empherical_prob],lr=LR)\n",
        "trying_cnt = torch.zeros(NUM_BANDIT, dtype=torch.int32)\n",
        "# running_rewards = torch.zeros(NUM_BANDIT)\n",
        "\n",
        "for _iter in range(T):\n",
        "    # building choices\n",
        "    \n",
        "    choice = torch.empty(n_batch, dtype=torch.int32)\n",
        "    use_emp = torch.FloatTensor(n_batch).uniform_(0,1) > eps\n",
        "    choice[use_emp] = torch.max(empherical_prob,0).indices\n",
        "    choice[~use_emp] = torch.randint(NUM_BANDIT, ((~use_emp).sum(),))\n",
        "    reward = pull_bandit(choice)\n",
        "    \n",
        "    for k,v in Counter(choice.data.tolist()).items():\n",
        "        running_rewards[k] += reward[k]*v\n",
        "        empherical_prob[k] = (empherical_prob[k]*trying_cnt[k] + reward[k]*v) / (trying_cnt[k] + v)\n",
        "        trying_cnt[k] += v\n",
        "\n",
        "    if (_iter+1) % LOG_INT == 0: print('[Iter {}] Running_reward = {} trying_cnt ={}'.format(_iter+1, np.array(running_rewards.data), np.array(trying_cnt.data)))\n",
        "\n",
        "\n",
        "print(\"Estimated Optimal Strategy : {}\".format(empherical_prob.max(0).indices))\n",
        "print(\"Answer Strategy : {}\".format(bandit.argmin()))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 1000] Running_reward = [ 479327.  120083.  390726.  357010.  119679. 1024317.  122613.  119086.] trying_cnt =[ 1326  1283  1349  1359  1410 90309  1267  1697]\n",
            "[Iter 2000] Running_reward = [ 480329.  121019.  391849.  357997.  120672. 1096804.  123597.  120031.] trying_cnt =[  2598   2472   2699   2571   2681 181544   2531   2904]\n",
            "[Iter 3000] Running_reward = [ 481313.  122028.  392828.  358956.  121623. 1168557.  124622.  121012.] trying_cnt =[  3825   3763   3915   3780   3907 272792   3835   4183]\n",
            "[Iter 4000] Running_reward = [ 482262.  123070.  393812.  359932.  122592. 1241268.  125624.  122006.] trying_cnt =[  5035   5034   5129   5028   5170 364099   5086   5419]\n",
            "[Iter 5000] Running_reward = [ 483299.  124068.  395146.  360850.  123648. 1311305.  126562.  122986.] trying_cnt =[  6318   6327   6910   6228   6477 454763   6311   6666]\n",
            "[Iter 6000] Running_reward = [ 484304.  125078.  396130.  361879.  124596. 1383849.  127630.  123949.] trying_cnt =[  7551   7596   8163   7510   7732 545925   7629   7894]\n",
            "[Iter 7000] Running_reward = [ 485302.  126019.  397088.  362899.  125592. 1455476.  128672.  124941.] trying_cnt =[  8832   8852   9414   8755   8976 637015   9019   9137]\n",
            "[Iter 8000] Running_reward = [ 486305.  126978.  397992.  363937.  126520. 1529021.  129672.  125994.] trying_cnt =[ 10125  10059  10572  10051  10199 728312  10277  10405]\n",
            "[Iter 9000] Running_reward = [ 487236.  127919.  398955.  364929.  127493. 1600065.  130668.  127050.] trying_cnt =[ 11330  11253  11797  11325  11422 819581  11561  11731]\n",
            "[Iter 10000] Running_reward = [ 488255.  128923.  399945.  365885.  128529. 1672130.  131612.  128079.] trying_cnt =[ 12615  12508  13068  12582  12723 910678  12815  13011]\n",
            "Estimated Optimal Strategy : 5\n",
            "Answer Strategy : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iew8ZWufOH77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}