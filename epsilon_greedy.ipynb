{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epsilon_greedy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaehanKim/reinforcement_learning_pytorch/blob/master/epsilon_greedy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjq659wfTKMQ",
        "colab_type": "text"
      },
      "source": [
        "# Epsilon Greedy Algorithm\n",
        "\n",
        "This notebook solves MAB(Multi-armed bandit) problem with epsilon greedy algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqkyVUd6ExME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2a_kUcZFF_M",
        "colab_type": "code",
        "outputId": "b4ee6d88-9934-40cc-9179-33c03a643698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "def pull_bandit(action, bandit):\n",
        "    if np.random.uniform(0,1,(1,)) > bandit[action]:\n",
        "        return 0\n",
        "    else: return 1\n",
        "\n",
        "# config\n",
        "NUM_BANDIT = 8\n",
        "T = 1000\n",
        "eps = .1\n",
        "LOG_INT = 100\n",
        "\n",
        "# define bandit\n",
        "bandit = np.random.uniform(0,1,(NUM_BANDIT,))\n",
        "\n",
        "# Initialize empherical prob ~ U(0,1)\n",
        "empherical_prob = np.random.uniform(0,1,(NUM_BANDIT,))\n",
        "\n",
        "trying_cnt = np.zeros(NUM_BANDIT)\n",
        "running_rewards = np.zeros(NUM_BANDIT)\n",
        "\n",
        "for _iter in range(T):\n",
        "    # building choice \n",
        "    if np.random.uniform(0,1,(1,)) > eps:\n",
        "        choice = empherical_prob.argmax()\n",
        "    else: \n",
        "        choice = np.random.randint(NUM_BANDIT)\n",
        "    reward = pull_bandit(choice, bandit)\n",
        "    \n",
        "    running_rewards[choice] += reward\n",
        "    empherical_prob[choice] = (empherical_prob[choice]*trying_cnt[choice] + reward) / (trying_cnt[choice] + 1)\n",
        "    trying_cnt[choice] += 1\n",
        "\n",
        "    if (_iter+1) % LOG_INT == 0: print('[Iter {}] Running_reward = {} trying_cnt ={}'.format(_iter+1, running_rewards, trying_cnt))\n",
        "\n",
        "\n",
        "print(\"Estimated Optimal Strategy : {}\".format(empherical_prob.argmax()))\n",
        "print(\"Answer Strategy : {}\".format(bandit.argmax()))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 100] Running_reward = [ 1.  5.  8.  0.  2.  0.  0. 49.] trying_cnt =[ 2.  8. 13.  0.  4.  2.  0. 71.]\n",
            "[Iter 200] Running_reward = [ 1.  5.  8. 58.  3.  0.  3. 68.] trying_cnt =[  2.  10.  14.  59.   5.   3.   5. 102.]\n",
            "[Iter 300] Running_reward = [  1.   5.  12. 151.   4.   0.   4.  68.] trying_cnt =[  2.  11.  18. 152.   6.   3.   6. 102.]\n",
            "[Iter 400] Running_reward = [  1.   5.  13. 240.   5.   0.   4.  70.] trying_cnt =[  2.  12.  19. 243.   8.   3.   8. 105.]\n",
            "[Iter 500] Running_reward = [  2.   6.  14. 326.   6.   0.   6.  74.] trying_cnt =[  3.  14.  20. 330.   9.   3.  12. 109.]\n",
            "[Iter 600] Running_reward = [  4.   8.  15. 415.   6.   0.   6.  74.] trying_cnt =[  6.  17.  21. 421.  10.   3.  13. 109.]\n",
            "[Iter 700] Running_reward = [  4.   8.  17. 506.   6.   0.   8.  74.] trying_cnt =[  6.  17.  24. 512.  11.   5.  15. 110.]\n",
            "[Iter 800] Running_reward = [  4.   8.  17. 598.   6.   0.   8.  75.] trying_cnt =[  6.  18.  26. 604.  12.   5.  18. 111.]\n",
            "[Iter 900] Running_reward = [  5.  11.  17. 682.   7.   0.   8.  76.] trying_cnt =[  8.  22.  26. 692.  17.   5.  18. 112.]\n",
            "[Iter 1000] Running_reward = [  5.  12.  18. 766.   7.   0.   8.  79.] trying_cnt =[  8.  26.  27. 778.  18.   6.  22. 115.]\n",
            "Estimated Optimal Strategy : 3\n",
            "Answer Strategy : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iew8ZWufOH77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}