{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "epsilon_greedy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaehanKim/reinforcement_learning_pytorch/blob/master/epsilon_greedy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjq659wfTKMQ",
        "colab_type": "text"
      },
      "source": [
        "# Epsilon Greedy Algorithm\n",
        "\n",
        "This notebook solves MAB(Multi-armed bandit) problem with epsilon greedy algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqkyVUd6ExME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def pull_bandit(action_tensor):\n",
        "    reward_mask = np.random.randn(action_tensor.size(0)) > bandit[action_tensor]\n",
        "    reward = torch.empty(action_tensor.size(0), dtype=torch.int32)\n",
        "    reward[reward_mask] = 1\n",
        "    reward[~reward_mask] = 0\n",
        "    return reward\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2a_kUcZFF_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "43261f65-3a08-490b-d48e-c717bc77d45a"
      },
      "source": [
        "# config\n",
        "NUM_BANDIT = 8\n",
        "T = 10000\n",
        "LR = 1e-3\n",
        "eps = .1\n",
        "n_batch = 100\n",
        "LOG_INT = 1000\n",
        "\n",
        "# define bandit\n",
        "bandit = np.random.randn(NUM_BANDIT)\n",
        "\n",
        "# learnable parameter\n",
        "empherical_prob = torch.nn.Parameter(torch.FloatTensor(NUM_BANDIT).uniform_(0,1))\n",
        "\n",
        "sgd = torch.optim.SGD([empherical_prob],lr=LR)\n",
        "trying_num = torch.zeros(NUM_BANDIT, dtype=torch.int32)\n",
        "running_rewards = torch.zeros(NUM_BANDIT)\n",
        "\n",
        "for _iter in range(T):\n",
        "    # building choices\n",
        "    loss = .0\n",
        "    \n",
        "    choice = torch.empty(n_batch, dtype=torch.int32)\n",
        "    use_emp = torch.FloatTensor(n_batch).uniform_(0,1) > eps\n",
        "    choice[use_emp] = torch.max(empherical_prob,0).indices\n",
        "    choice[~use_emp] = torch.randint(NUM_BANDIT, ((~use_emp).sum(),))\n",
        "    reward = pull_bandit(choice)\n",
        "    sgd.zero_grad()\n",
        "    \n",
        "    for k,v in Counter(choice.data.tolist()).items():\n",
        "        trying_num[k] += v\n",
        "        running_rewards[k] += reward[k]*v\n",
        "        loss += -torch.log(empherical_prob[k])*reward[k]*v\n",
        "    \n",
        "    loss.backward()\n",
        "    sgd.step()\n",
        "\n",
        "    if (_iter+1) % LOG_INT == 0: print('[Iter {}] Loss = {:.5f} Running_reward = {} trying_cnt ={}'.format(_iter+1, loss.item(), np.array(running_rewards.data), np.array(trying_num.data)))\n",
        "\n",
        "print(\"Estimated Optimal Strategy : {}\".format(empherical_prob.max(0).indices))\n",
        "print(\"Answer Strategy : {}\".format(bandit.argmin()))\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 1000] Loss = -232.10327 Running_reward = [ 1151.  1058. 83248.  1122.  1119.  1141.  1044.  1215.] trying_cnt =[ 1272  1184 91338  1262  1235  1247  1173  1289]\n",
            "[Iter 2000] Loss = -288.24347 Running_reward = [  2300.   2189. 166030.   2253.   2272.   2316.   2201.   2299.] trying_cnt =[  2552   2416 182568   2521   2486   2532   2444   2481]\n",
            "[Iter 3000] Loss = -291.41095 Running_reward = [  3447.   3326. 249596.   3366.   3373.   3447.   3231.   3432.] trying_cnt =[  3825   3697 273906   3773   3684   3790   3601   3724]\n",
            "[Iter 4000] Loss = -307.73984 Running_reward = [  4604.   4479. 332037.   4537.   4481.   4491.   4412.   4619.] trying_cnt =[  5103   4982 365098   5059   4890   4930   4898   5040]\n",
            "[Iter 5000] Loss = -324.03775 Running_reward = [  5761.   5566. 416357.   5671.   5634.   5548.   5525.   5739.] trying_cnt =[  6385   6180 456398   6312   6165   6115   6145   6300]\n",
            "[Iter 6000] Loss = -318.77130 Running_reward = [  6857.   6723. 500039.   6858.   6899.   6673.   6564.   6890.] trying_cnt =[  7624   7470 547522   7597   7532   7368   7322   7565]\n",
            "[Iter 7000] Loss = -329.48325 Running_reward = [  7917.   7818. 583391.   7964.   8072.   7806.   7714.   8021.] trying_cnt =[  8798   8664 638944   8802   8802   8617   8576   8797]\n",
            "[Iter 8000] Loss = -344.70193 Running_reward = [  9047.   9015. 665796.   9086.   9147.   8919.   8812.   9101.] trying_cnt =[ 10056   9972 730296  10042  10003   9839   9804   9988]\n",
            "[Iter 9000] Loss = -349.09091 Running_reward = [ 10184.  10149. 749557.  10256.  10277.  10040.   9956.  10250.] trying_cnt =[ 11309  11242 821518  11316  11251  11067  11054  11243]\n",
            "[Iter 10000] Loss = -10.89666 Running_reward = [ 11328.  11268. 832862.  11349.  11388.  11158.  11078.  11375.] trying_cnt =[ 12608  12482 912836  12504  12480  12304  12310  12476]\n",
            "Estimated Optimal Strategy : 2\n",
            "Answer Strategy : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iew8ZWufOH77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}