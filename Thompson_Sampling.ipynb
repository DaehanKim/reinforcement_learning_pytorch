{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thompson Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaehanKim/reinforcement_learning_tutorial/blob/master/Thompson_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb0w_fJ49gKj",
        "colab_type": "text"
      },
      "source": [
        "## Thompson Sampling\n",
        "\n",
        "This code solves Multi-armed bandit problem using Thompson sampling.\n",
        "When arm $i$ has a bernouli reward, this method assumes Beta Distribution $\\beta (\\alpha_i, \\beta_i)$ as a prior distribution of reward mean $\\mu_i$. As the number of trials increases, $\\mu_i$'s posterior distribution becomes sharp. Choices are made with a maximum value of sampled $\\hat{\\mu}_i$ from corresponding Beta Distribution $\\beta (\\alpha_i, \\beta_i)$ among all $i$.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmo2rQu59b9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yivweDrw_P4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "5c93fe87-e955-46c8-f359-aad090aa69b0"
      },
      "source": [
        "#configs\n",
        "NUM_BANDIT = 10\n",
        "NUM_TRIAL = 100\n",
        "LOG_INT = 10\n",
        "\n",
        "# set initial alpha and beta for each arm\n",
        "alpha = np.full(NUM_BANDIT,2)\n",
        "beta = np.full(NUM_BANDIT,2)\n",
        "\n",
        "# initialize arm probabilities\n",
        "bandit = np.random.uniform(0,1,(NUM_BANDIT,))\n",
        "n_trial = np.zeros(NUM_BANDIT)\n",
        "\n",
        "for _iter in range(NUM_TRIAL):\n",
        "    # choose an arm\n",
        "    estimated_means = np.array([np.random.beta(alpha[i],beta[i],1)[0] for i in range(NUM_BANDIT)])\n",
        "    choice = estimated_means.argmax()\n",
        "    reward = np.random.binomial(1,bandit[choice],1)[0]\n",
        "    \n",
        "    # update posterior parameter for mu_i\n",
        "    alpha[choice] += reward\n",
        "    beta[choice] += (1-reward)\n",
        "    n_trial[choice] += 1\n",
        "\n",
        "    if (_iter+1) % LOG_INT == 0 : print('[Iter {}] mu={} / num_trial={}'.format(_iter + 1, estimated_means, n_trial))\n",
        "\n",
        "print(\"estimated optimal strategy : {}\".format(estimated_means.argmax()))\n",
        "print(\"actual optimal strategy : {}\".format(bandit.argmax()))\n",
        "print(\"actual arm probabilities : {}\".format(bandit))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 10] mu=[0.43864304 0.81086046 0.30966778 0.60480758 0.24216787 0.06146544\n",
            " 0.48650597 0.26786838 0.72462738 0.51408369] / num_trial=[0. 1. 4. 0. 0. 0. 0. 0. 3. 2.]\n",
            "[Iter 20] mu=[0.75353077 0.46049954 0.33366378 0.51690514 0.216198   0.48146588\n",
            " 0.43621068 0.46571967 0.55206524 0.64314623] / num_trial=[3. 1. 4. 1. 0. 0. 1. 1. 4. 5.]\n",
            "[Iter 30] mu=[0.27491467 0.82304714 0.5034348  0.26162214 0.69903014 0.56328133\n",
            " 0.17012965 0.48437448 0.52865009 0.81940935] / num_trial=[ 3.  2.  4.  1.  0.  3.  1.  1.  5. 10.]\n",
            "[Iter 40] mu=[0.49661653 0.43223508 0.19323368 0.40243233 0.50081837 0.857376\n",
            " 0.40524399 0.55352872 0.63944349 0.68939469] / num_trial=[ 3.  2.  4.  1.  1.  4.  1.  1.  6. 17.]\n",
            "[Iter 50] mu=[0.17487334 0.59775449 0.5303887  0.6845986  0.45424232 0.45778087\n",
            " 0.29316937 0.40691615 0.48164875 0.79718403] / num_trial=[ 3.  2.  4.  1.  1.  4.  2.  1.  7. 25.]\n",
            "[Iter 60] mu=[0.23999114 0.15131804 0.59547387 0.36717146 0.31685338 0.49286082\n",
            " 0.43797097 0.19570235 0.79209681 0.92964951] / num_trial=[ 3.  2.  5.  1.  1.  4.  2.  1.  7. 34.]\n",
            "[Iter 70] mu=[0.5558777  0.23827322 0.55698058 0.40031377 0.43599578 0.36468446\n",
            " 0.16793957 0.4410822  0.69912845 0.88364044] / num_trial=[ 3.  2.  5.  1.  2.  4.  2.  1.  8. 42.]\n",
            "[Iter 80] mu=[0.21860228 0.34439749 0.25811414 0.30069343 0.88174896 0.74186586\n",
            " 0.30134151 0.75200798 0.47948979 0.93519647] / num_trial=[ 3.  2.  5.  1.  5.  4.  2.  1.  8. 49.]\n",
            "[Iter 90] mu=[0.4475394  0.15412469 0.26950033 0.41033697 0.9028091  0.61613642\n",
            " 0.08768343 0.42071015 0.6120654  0.88509516] / num_trial=[ 3.  2.  5.  1.  6.  4.  2.  1.  8. 58.]\n",
            "[Iter 100] mu=[0.55049932 0.45677997 0.28171246 0.17838403 0.86151533 0.59799197\n",
            " 0.29812886 0.19981749 0.74373358 0.94205577] / num_trial=[ 3.  2.  5.  1.  7.  4.  2.  1.  8. 67.]\n",
            "estimated optimal strategy : 9\n",
            "actual optimal strategy : 9\n",
            "actual arm probabilities : [0.3007574  0.18230181 0.25813315 0.71082358 0.81273295 0.34368277\n",
            " 0.31457228 0.54748345 0.67689703 0.94306355]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFzNRQ4VBFul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}